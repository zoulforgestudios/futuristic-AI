
  <!DOCTYPE html>
  <html lang="en">
    <head>
      <meta charset="UTF-8" />
      <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <title>Futuristic AI App Design</title>
    </head>

    <body>
      <!-- place anywhere inside your page, e.g. topbar actions -->
<button id="bb_mic" class="btn">üéôÔ∏è</button>

      <div id="root"></div>
      <script type="module" src="/src/main.tsx"></script>
      <script>
(function(){if(!window.chatbase||window.chatbase("getState")!=="initialized"){window.chatbase=(...arguments)=>{if(!window.chatbase.q){window.chatbase.q=[]}window.chatbase.q.push(arguments)};window.chatbase=new Proxy(window.chatbase,{get(target,prop){if(prop==="q"){return target.q}return(...args)=>target(prop,...args)}})}const onLoad=function(){const script=document.createElement("script");script.src="https://www.chatbase.co/embed.min.js";script.id="iQ-Hm5SsVmfUBOETQ0QZd";script.domain="www.chatbase.co";document.body.appendChild(script)};if(document.readyState==="complete"){onLoad()}else{window.addEventListener("load",onLoad)}})();
</script>
<script>
  // Call this when a new agent reply text arrives
  function zoulSpeak(text){
    try{
      const u = new SpeechSynthesisUtterance(text);
      u.rate = 1.05; u.pitch = 1.0; u.volume = 1.0;
      // pick a voice if you want: const v = speechSynthesis.getVoices().find(v => v.name.includes('English'));
      // if (v) u.voice = v;
      speechSynthesis.cancel(); // stop any ongoing
      speechSynthesis.speak(u);
    }catch(e){ console.warn('TTS unavailable', e); }
  }
  // Example: zoulSpeak("Zoul online.");
</script>
<script>
/* ========= ZoulForge Voice I/O ========= */
(function(){
  const hasSTT = ('webkitSpeechRecognition' in window) || ('SpeechRecognition' in window);
  const synth   = window.speechSynthesis;
  const Rec     = window.SpeechRecognition || window.webkitSpeechRecognition;

  let rec = null;
  let listening = false;
  let speaking  = false;
  let selectedVoice = null;

  // Elements your site already has
  const $ = s => document.querySelector(s);
  const input  = $('#composerInput') || $('#userInput');
  const send   = $('#composerSend') || $('#sendBtn');
  const micBtn = $('#bb_mic') || $('#heroMic');
  const chatBox= $('#chatBox');

  // ---- TTS ----
  function pickVoice(preferred = ['Microsoft Neerja Online (Natural) - English (India)',
                                  'Google ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä',
                                  'Google ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä (‡§≠‡§æ‡§∞‡§§)',
                                  'Microsoft Heera - English (India)',
                                  'Google US English']){
    const voices = synth.getVoices();
    if (!voices.length) return null;
    for (const name of preferred){
      const v = voices.find(v=>v.name === name);
      if (v) return v;
    }
    // fallback same-language preferred
    return voices.find(v => /en-IN|hi-IN|en-|hi-/.test(v.lang)) || voices[0];
  }

  function speak(text, opts={}){
    if (!synth) return;
    if (!text) return;
    // stop any current speech
    synth.cancel();
    speaking = true;

    const utter = new SpeechSynthesisUtterance(text);
    selectedVoice = selectedVoice || pickVoice();
    if (selectedVoice) utter.voice = selectedVoice;
    utter.rate   = opts.rate ?? 1.0;   // 0.8‚Äì1.2 sounds best
    utter.pitch  = opts.pitch ?? 1.0;  // 0.8‚Äì1.2
    utter.volume = 1.0;

    utter.onend = () => { speaking = false; };
    utter.onerror = () => { speaking = false; };
    synth.speak(utter);
  }

  // ---- STT ----
  function ensureRecognizer(){
    if (!hasSTT) return null;
    if (rec) return rec;
    rec = new Rec();
    rec.lang = 'en-IN';        // change if you prefer
    rec.interimResults = true;
    rec.continuous = false;

    let finalText = '';

    rec.onstart = () => { listening = true; document.body.classList.add('zf-listening'); pulseMic(true); };
    rec.onresult = (e) => {
      let interim = '';
      for (let i = e.resultIndex; i < e.results.length; i++){
        const t = e.results[i][0].transcript;
        if (e.results[i].isFinal) finalText += t;
        else interim += t;
      }
      if (input){
        input.value = (finalText || interim).trim();
      }
    };
    rec.onerror = () => { listening = false; document.body.classList.remove('zf-listening'); pulseMic(false); };
    rec.onend = () => {
      listening = false;
      document.body.classList.remove('zf-listening');
      pulseMic(false);
      // auto-send if we captured speech
      if (input && input.value.trim()){
        send?.click();
      }
    };
    return rec;
  }

  function startListening(){
    if (!hasSTT) { toast('Speech recognition not supported in this browser'); return; }
    if (speaking) synth.cancel(); // barge-in: stop TTS when user speaks
    const r = ensureRecognizer();
    try { r && r.start(); } catch(e){ /* ignore "already started" */ }
  }
  function stopListening(){
    try { rec && rec.stop(); } catch(e){}
  }

  function pulseMic(on){
    if (!micBtn) return;
    micBtn.style.outline = on ? '2px solid #8a5cff' : 'none';
    micBtn.style.boxShadow = on ? '0 0 18px rgba(138,92,255,.45)' : 'none';
  }

  function toast(msg){
    const t = document.createElement('div');
    t.textContent = msg;
    Object.assign(t.style,{
      position:'fixed',left:'50%',bottom:'22px',transform:'translateX(-50%)',
      background:'#141825',border:'1px solid #2a2f3a',color:'#fff',
      padding:'8px 12px',borderRadius:'12px',zIndex:200
    });
    document.body.appendChild(t); setTimeout(()=>t.remove(),1400);
  }

  // ---- Glue: hook into your existing send flow ----
  if (send){
    // Monkey-patch the click so we can TTS the assistant reply afterwards.
    const origClick = send.onclick;
    send.onclick = async function(ev){
      // 1) read user text
      const text = input?.value?.trim();
      if (!text) { origClick && origClick.call(this, ev); return; }

      // 2) append user bubble if your code doesn‚Äôt already
      ensureBubble('user', text);

      // 3) call your agent ‚Üí get reply
      const reply = await callAgent(text).catch(()=> 'Sorry, I had trouble responding.');
      ensureBubble('assistant', reply);

      // 4) speak it
      speak(reply);

      // 5) clear input if your code doesn‚Äôt
      if (input) input.value = '';

      // Don‚Äôt call origClick to avoid double sending. If you need original side-effects,
      // move them here explicitly.
    };
  }

  // If you want the mic button to toggle listen/stop
  micBtn?.addEventListener('click', ()=>{
    if (!listening) startListening(); else stopListening();
  });

  function ensureBubble(who, text){
    if (!chatBox) return;
    const wrap = document.createElement('div');
    wrap.className = 'msg ' + (who==='user'?'user':'assistant');
    wrap.innerHTML = `<div class="avatar"></div><div class="bubble"></div>`;
    wrap.querySelector('.bubble').textContent = text;
    chatBox.appendChild(wrap);
    chatBox.scrollTop = chatBox.scrollHeight;
  }

  // ---- Replace this with your actual Chatbase/agent call ----
  async function callAgent(userText){
async function callAgent(userText){
  const r = await fetch("https://api.chatbase.co/api/v1/agents/iQ-Hm5SsVmfUBOETQ0QZd/message", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "Authorization": "530b1db0-a93c-444f-89b1-115bee529e58"
    },
    body: JSON.stringify({ message: userText, stream: false })
  });
  const j = await r.json();
  return j.reply || j.output || '‚Ä¶';
}

    return new Promise(res => setTimeout(()=> res(`Roger: ${userText}`), 350));
  }

  // Preload voices (Chrome loads async)
  if (synth && typeof synth.onvoiceschanged !== 'undefined'){
    synth.onvoiceschanged = () => { selectedVoice = selectedVoice || pickVoice(); };
  }
})();
</script>


    </body>
  </html>
  